# -*- coding: utf-8 -*-
"""Similarity-Karmawibhangga.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4GO8RwEsZxlTN5sSzoetlpSp7SVR-r7

## Setup Libraries
"""

import os
import json
import h5py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from PIL import Image
from pathlib import Path

from collections import defaultdict
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from sklearn.metrics.pairwise import cosine_similarity

# load from google drive
from google.colab import drive
drive.mount('/content/drive')

"""##Load the Raw Data

Fungsi berikut dijalankan untuk memeberikan hasil gambar asli dari dataset yang diambil secara acak dan data csv berupa narasi cerita
"""

# Data Images
def load_images(directory):
    images = []
    filenames = []
    for filename in os.listdir(directory):
        try:
            img_path = os.path.join(directory, filename)
            img = Image.open(img_path).convert('RGB')
            images.append(img)
            filenames.append(filename)
        except Exception as e:
            print(f"Error loading {filename}: {e}")
    return images, filenames

# Inisiasi path folder dataset
IMAGE_DIR = "/content/drive/MyDrive/Colab Notebooks/Capstone/image all"
images, filenames = load_images(IMAGE_DIR)

# Hasil total gambar karmawibhangga
print(f"Total gambar karmawibhangga: {len(images)}")

# Menampilkan visualisasi gambar yang diambil 5 acak dari IMAGE DIR
plt.figure(figsize=(20, 10))
for i in range(min(5, len(images))):
    plt.subplot(1, 5, i + 1)
    plt.imshow(images[i])
    plt.axis('off')
    plt.title(filenames[i], fontsize=8)
plt.tight_layout()
plt.show()

"""## Data Exploration

**Explore image** Informasi yang sudah didapatkan saat melakuakn laod raw data gambar, ada total **160 gambar** relief karmawibhangga, tidak ada file rusak dan kosong, informasi lain adalah resolusi gambar yang berbeda beda
"""

# menghitung jumlah resolusi gambar
def count_image_by_resolution_flat(IMAGE_DIR):
    resolution_counts = defaultdict(int)

    for image_name in os.listdir(IMAGE_DIR):
        image_path = os.path.join(IMAGE_DIR, image_name)
        if os.path.isfile(image_path):
            try:
                with Image.open(image_path) as img:
                    resolution = img.size  # (width, height)
                    resolution_counts[resolution] += 1
            except Exception as e:
                print(f"Gagal membaca {image_path}: {e}")

    print("\nJumlah gambar berdasarkan resolusi:")
    for resolution, count in resolution_counts.items():
        print(f"- {resolution}: {count} gambar")

    return resolution_counts

# Contoh panggilan fungsi
count_image_by_resolution_flat(IMAGE_DIR)

"""Karena dataset relatif kecil maka kami bisa langsung melihat dan menganalisis secara manual mengenai kondisi dataset ini, ternyata dataset yang missing value memiliki bentuk gambar yang tidak bisa diartikan/ ukirannya memang tidak jelas. sehingga pada tahapan selanjutnya kami tidak akan menghapus data tersebut, namun menambahkan keterangan bahwa gambar tersebut tidak adapat diartikan

## Data Preparation

Data yang akan digunakan adalah dataset gambar saja dengan menggunakan algoritma **image similarity**
1. Preprocessing gamabr dengan resize gambar, konversi ke array karena akan menggunakan keras tensorflow, dan menyesuaikan skala pixel
"""

image_size = (224, 224)
valid_ext = ['.jpg', '.jpeg', '.png']

def preprocess_image(img_path, target_size):
    img = load_img(img_path, target_size=target_size)  # resize
    img_array = img_to_array(img)                      # convert ke array
    img_array = np.expand_dims(img_array, axis=0)     # tambah dim batch
    img_array = preprocess_input(img_array)            # preprocess ResNet50
    return img_array

image_arrays = []
image_names = []

for filename in os.listdir(IMAGE_DIR):
    if any(filename.lower().endswith(ext) for ext in valid_ext):
        file_path = os.path.join(IMAGE_DIR, filename)
        if os.path.isfile(file_path):
            try:
                img_arr = preprocess_image(file_path, image_size)
                image_arrays.append(img_arr[0])
                image_names.append(filename)
            except Exception as e:
                print(f"Error processing {file_path}: {e}")

image_data = np.array(image_arrays)
print(f"Total images processed: {len(image_data)}")
print(f"Shape image_data: {image_data.shape}")

"""gambarnya terlihat aneh (warna aneh, terlalu gelap), itu karena preprocess_input() dari ResNet mengubah nilai piksel menjadi skala khusus. Saat ingin ditampilkan, normalisasi balik dengan (img + 1)/2 atau cukup pakai plt.imshow(img.astype("uint8")) sebelum preprocess_input untuk visualisasi asli."""

for i in range(5):
    plt.imshow((image_data[i] + 1) / 2)
    plt.title(image_names[i])
    plt.axis('off')
    plt.show()

"""## Feature Extraction

- Menggunakan model CNN pretrained (ResNet50 tanpa bagian klasifikasi paling atas) sebagai feature extractor.

- Input: image_data → Output: array fitur untuk tiap gambar

- Fitur ini adalah representasi gambar dalam bentuk angka yang bisa dibandingkan satu sama lain.
"""

model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224,224,3))
features = model.predict(image_data, batch_size=32, verbose=1)
print(f"Features shape: {features.shape}")

"""## Save fitur dan nama file"""

with h5py.File('features.h5', 'w') as h5f:
    h5f.create_dataset('features', data=features)
    # Simpan nama file sebagai string array dan convert ke bytes
    dt = h5py.string_dtype(encoding='utf-8')
    h5f.create_dataset('image_names', data=np.array(image_names, dtype=dt))

# Read data dari H5
with h5py.File('features.h5', 'r') as h5f:
    features = h5f['features'][:]        # baca semua fitur
    image_names = h5f['image_names'][:] # baca semua nama gambar sebagai bytes
    image_names = [name.decode('utf-8') for name in image_names]  # decode ke string

"""## Similarity Search

- Pada image similarity berbasis feature embedding dengan ResNet50 + cosine similarity.

- Teknik ini Tidak ada proses training supervised → tidak melatih model klasifikasi, sehingga tidak ada kebutuhan validasi performa menggunakan label.

- Model ResNet50 digunakan hanya sebagai feature extractor, yang sudah dilatih pada ImageNet.

- Semua gambar dianggap sebagai "database referensi" yang akan dicari kemiripannya dengan input baru dari user.
"""

# Fungsi berikut untuk mencari gambar yang mirip
def find_similar_images(query_feature, features, top_k=5):
    similarities = cosine_similarity(query_feature.reshape(1, -1), features)
    indices = similarities[0].argsort()[-top_k:][::-1]
    return indices, similarities[0][indices]

# testing
def preprocess_query_image(img_path, target_size=image_size):
    img = load_img(img_path, target_size=target_size)
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    return img_array

query_img_path = '/content/drive/MyDrive/Colab Notebooks/Capstone/image all/018-Attending-to-Sick-Parents-leads-to-Little-Illness-Original.jpg'
query_img = preprocess_query_image(query_img_path)
query_feature = model.predict(query_img)

indices, scores = find_similar_images(query_feature, features, top_k=3)
print("Top 3 similar images:")
for i, idx in enumerate(indices):
    print(f"{i+1}. {image_names[idx]} with similarity score: {scores[i]:.4f}")

# Tampilkan gambar yang relevan
for idx in indices:
    img_path = os.path.join(IMAGE_DIR, image_names[idx])
    img = load_img(img_path)
    plt.imshow(img)
    plt.title(image_names[idx])
    plt.axis('off')
    plt.show()

"""## Membuat dictionary narasi

Mapping Narasi/load data narasi karmawibhangga dan menyimpan dalam struktur dictionary
"""

narrative_dict = {}

for _, row in df_narasi.iterrows():
    narrative_dict[row['filename']] = {
        'tema': row['Tema'],
        'narasi': row['Narasi'],
        'makna_moral': row['Makna moral']
    }

# Check apakah struktur mapping sudah benar atau belum. Lihat 3 item pertama
for filename, content in list(narrative_dict.items())[:3]:
    print(f"Filename: {filename}")
    if isinstance(content, dict):
        print(f"Tema       : {content.get('tema')}")
        print(f"Narasi     : {content.get('narasi')}")
        print(f"Makna Moral: {content.get('makna_moral')}")
    else:
        # Jika hanya narasi saja
        print(f"Narasi     : {content}")
    print("-" * 50)

print(f"Jumlah narasi: {len(narrative_dict)}")

# Simpan dalam bentuk JSON
with open('narrative_dict.json', 'w', encoding='utf-8') as f:
    json.dump(narrative_dict, f, ensure_ascii=False, indent=2)